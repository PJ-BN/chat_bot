{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc5f0c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd1328c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a14b276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intends': [{'tag': ['greating'],\n",
       "   'question': ['hello', \"what's up\", 'hey', 'hi', 'hey there'],\n",
       "   'answer': ['Hi!']},\n",
       "  {'tag': ['name'],\n",
       "   'question': ['what is your name',\n",
       "    'introduce',\n",
       "    'introduce yourself',\n",
       "    'who',\n",
       "    'name',\n",
       "    'who are you'],\n",
       "   'answer': ['My name is Prajwal Bhandari']},\n",
       "  {'tag': ['health'],\n",
       "   'question': ['how are you', 'fine', 'how is your health', 'health'],\n",
       "   'answer': [\"Yes I'm fine!\"]},\n",
       "  {'tag': ['situation'],\n",
       "   'question': ['how is it going',\n",
       "    'are you okay',\n",
       "    'how do you do',\n",
       "    'are you good',\n",
       "    'how is your study going on'],\n",
       "   'answer': ['Thikai!']},\n",
       "  {'tag': ['address'],\n",
       "   'question': ['where do you live',\n",
       "    'home',\n",
       "    'where is your home',\n",
       "    'what is your address',\n",
       "    'address'],\n",
       "   'answer': ['I live in Tilottama, Nepal']},\n",
       "  {'tag': ['college'],\n",
       "   'question': ['where do you study',\n",
       "    'college',\n",
       "    'which university do you study',\n",
       "    'university'],\n",
       "   'answer': ['Nepathya College affiliated to Tribhuvan University']},\n",
       "  {'tag': ['education'],\n",
       "   'question': ['what do you study', 'subject', 'what is your major'],\n",
       "   'answer': ['Bachelor in Computer Application(BCA)']},\n",
       "  {'tag': ['goodbye'],\n",
       "   'question': ['bye',\n",
       "    'goodbye',\n",
       "    'later',\n",
       "    'see you next time',\n",
       "    'see you later',\n",
       "    'take care'],\n",
       "   'answer': ['Bye!!']},\n",
       "  {'tag': ['location'],\n",
       "   'question': ['your current location', 'where', 'location', 'where are you'],\n",
       "   'answer': ['Ghar']}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = open('data.json')\n",
    "json_data = json.load(d)\n",
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9892d71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "what's up\n",
      "hey\n",
      "hi\n",
      "hey there\n",
      "what is your name\n",
      "introduce\n",
      "introduce yourself\n",
      "who\n",
      "name\n",
      "who are you\n",
      "how are you\n",
      "fine\n",
      "how is your health\n",
      "health\n",
      "how is it going\n",
      "are you okay\n",
      "how do you do\n",
      "are you good\n",
      "how is your study going on\n",
      "where do you live\n",
      "home\n",
      "where is your home\n",
      "what is your address\n",
      "address\n",
      "where do you study\n",
      "college\n",
      "which university do you study\n",
      "university\n",
      "what do you study\n",
      "subject\n",
      "what is your major\n",
      "bye\n",
      "goodbye\n",
      "later\n",
      "see you next time\n",
      "see you later\n",
      "take care\n",
      "your current location\n",
      "where\n",
      "location\n",
      "where are you\n"
     ]
    }
   ],
   "source": [
    "ques = []\n",
    "document = []\n",
    "classes = []\n",
    "\n",
    "\n",
    "for intents in json_data['intends']:\n",
    "    for question in intents['question']:\n",
    "        print(question)\n",
    "        \n",
    "        doc = nlp(question.lower())\n",
    "        text = [token.lemma_ for token in doc]\n",
    "        \n",
    "       \n",
    "        ques.append(text)\n",
    "        document.append(( text, intents['tag']))\n",
    "\n",
    "        \n",
    "        \n",
    "    for question in intents['tag']:\n",
    "#         print(question)\n",
    "        if question not in list(classes):\n",
    "            classes.append(question)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb052c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['your', 'current', 'location'], ['location']),\n",
       " (['how', 'be', 'it', 'go'], ['situation']),\n",
       " (['hello'], ['greating']),\n",
       " (['hi'], ['greating']),\n",
       " (['home'], ['address']),\n",
       " (['where', 'do', 'you', 'live'], ['address']),\n",
       " (['hey'], ['greating']),\n",
       " (['where', 'be', 'you'], ['location']),\n",
       " (['how', 'do', 'you', 'do'], ['situation']),\n",
       " (['university'], ['college']),\n",
       " (['see', 'you', 'next', 'time'], ['goodbye']),\n",
       " (['take', 'care'], ['goodbye']),\n",
       " (['be', 'you', 'good'], ['situation']),\n",
       " (['name'], ['name']),\n",
       " (['what', 'be', 'your', 'address'], ['address']),\n",
       " (['college'], ['college']),\n",
       " (['what', 'be', 'your', 'name'], ['name']),\n",
       " (['how', 'be', 'your', 'study', 'go', 'on'], ['situation']),\n",
       " (['be', 'you', 'okay'], ['situation']),\n",
       " (['what', 'be', 'up'], ['greating']),\n",
       " (['later'], ['goodbye']),\n",
       " (['how', 'be', 'your', 'health'], ['health']),\n",
       " (['introduce', 'yourself'], ['name']),\n",
       " (['see', 'you', 'later'], ['goodbye']),\n",
       " (['who'], ['name']),\n",
       " (['which', 'university', 'do', 'you', 'study'], ['college']),\n",
       " (['bye'], ['goodbye']),\n",
       " (['fine'], ['health']),\n",
       " (['where'], ['location']),\n",
       " (['how', 'be', 'you'], ['health']),\n",
       " (['where', 'be', 'your', 'home'], ['address']),\n",
       " (['what', 'be', 'your', 'major'], ['education']),\n",
       " (['introduce'], ['name']),\n",
       " (['health'], ['health']),\n",
       " (['location'], ['location']),\n",
       " (['goodbye'], ['goodbye']),\n",
       " (['subject'], ['education']),\n",
       " (['where', 'do', 'you', 'study'], ['college']),\n",
       " (['who', 'be', 'you'], ['name']),\n",
       " (['hey', 'there'], ['greating']),\n",
       " (['address'], ['address']),\n",
       " (['what', 'do', 'you', 'study'], ['education'])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(document)\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcff9f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['greating',\n",
       " 'name',\n",
       " 'health',\n",
       " 'situation',\n",
       " 'address',\n",
       " 'college',\n",
       " 'education',\n",
       " 'goodbye',\n",
       " 'location']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3e12b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['see you next time',\n",
       " 'take care',\n",
       " 'be you good',\n",
       " 'name',\n",
       " 'what be your address',\n",
       " 'college',\n",
       " 'what be your name',\n",
       " 'how be your study go on',\n",
       " 'be you okay',\n",
       " 'what be up',\n",
       " 'later',\n",
       " 'how be your health',\n",
       " 'introduce yourself',\n",
       " 'see you later',\n",
       " 'who',\n",
       " 'which university do you study',\n",
       " 'bye',\n",
       " 'fine',\n",
       " 'where',\n",
       " 'how be you',\n",
       " 'where be your home',\n",
       " 'what be your major',\n",
       " 'introduce',\n",
       " 'health',\n",
       " 'location',\n",
       " 'goodbye',\n",
       " 'subject',\n",
       " 'where do you study',\n",
       " 'who be you',\n",
       " 'hey there',\n",
       " 'address',\n",
       " 'what do you study']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques = [\" \".join(i[0]) for i in document]\n",
    "ques[10:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf778b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['location',\n",
       " 'situation',\n",
       " 'greating',\n",
       " 'greating',\n",
       " 'address',\n",
       " 'address',\n",
       " 'greating',\n",
       " 'location',\n",
       " 'situation',\n",
       " 'college',\n",
       " 'goodbye',\n",
       " 'goodbye',\n",
       " 'situation',\n",
       " 'name',\n",
       " 'address',\n",
       " 'college',\n",
       " 'name',\n",
       " 'situation',\n",
       " 'situation',\n",
       " 'greating',\n",
       " 'goodbye',\n",
       " 'health',\n",
       " 'name',\n",
       " 'goodbye',\n",
       " 'name',\n",
       " 'college',\n",
       " 'goodbye',\n",
       " 'health',\n",
       " 'location',\n",
       " 'health',\n",
       " 'address',\n",
       " 'education',\n",
       " 'name',\n",
       " 'health',\n",
       " 'location',\n",
       " 'goodbye',\n",
       " 'education',\n",
       " 'college',\n",
       " 'name',\n",
       " 'greating',\n",
       " 'address',\n",
       " 'education']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = [\"\".join(i[1]) for i in document]\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f48fcb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Documents\\ML\\NLP\\bot\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "\n",
    "train_one_hot = ohe.fit_transform(np.array(ans).reshape(-1,1))\n",
    "train_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23e96580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ques = ques[:30]\n",
    "train_ans = train_one_hot[:30]\n",
    "\n",
    "val_ques = ques[30:]\n",
    "val_ans = train_one_hot[30:]\n",
    "\n",
    "len(train_ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5dc0917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 9), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets = tf.data.Dataset.from_tensor_slices((train_ques, train_ans)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_datasets = tf.data.Dataset.from_tensor_slices((val_ques, val_ans)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75992ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 10000\n",
    "output_length = 200\n",
    "\n",
    "text_vector = layers.TextVectorization(max_tokens = max_vocab+1 , output_sequence_length=output_length )\n",
    "\n",
    "text_embed = layers.Embedding(max_vocab, 128, mask_zero=True)\n",
    "\n",
    "text_vector.adapt(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dea4225b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_28\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " text_vectorization (TextVector  (None, 200)         0           ['input_13[0][0]']               \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 200, 128)     1280000     ['text_vectorization[12][0]']    \n",
      "                                                                                                  \n",
      " dense_70 (Dense)               (None, 200, 264)     34056       ['embedding[12][0]']             \n",
      "                                                                                                  \n",
      " LSTM_layer (Bidirectional)     (None, 1024)         3182592     ['dense_70[0][0]']               \n",
      "                                                                                                  \n",
      " GRU_layer (Bidirectional)      (None, 1024)         2390016     ['dense_70[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 1024)         0           ['LSTM_layer[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)           (None, 1024)         0           ['GRU_layer[0][0]']              \n",
      "                                                                                                  \n",
      " dense_71 (Dense)               (None, 264)          270600      ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_73 (Dense)               (None, 264)          270600      ['dropout_25[0][0]']             \n",
      "                                                                                                  \n",
      " dense_72 (Dense)               (None, 9)            2385        ['dense_71[0][0]']               \n",
      "                                                                                                  \n",
      " dense_74 (Dense)               (None, 9)            2385        ['dense_73[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 18)           0           ['dense_72[0][0]',               \n",
      "                                                                  'dense_74[0][0]']               \n",
      "                                                                                                  \n",
      " dense_75 (Dense)               (None, 264)          5016        ['concatenate_8[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_26 (Dropout)           (None, 264)          0           ['dense_75[0][0]']               \n",
      "                                                                                                  \n",
      " dense_76 (Dense)               (None, 264)          69960       ['dropout_26[0][0]']             \n",
      "                                                                                                  \n",
      " dense_77 (Dense)               (None, 264)          69960       ['dense_76[0][0]']               \n",
      "                                                                                                  \n",
      " dense_78 (Dense)               (None, 9)            2385        ['dense_77[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,579,955\n",
      "Trainable params: 7,579,955\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(shape = (1,), dtype = tf.string)\n",
    "vector = text_vector(inputs)\n",
    "embed = text_embed(vector)\n",
    "first_dense = layers.Dense(264, activation = \"relu\")(embed)\n",
    "lstm_rnn = layers.Bidirectional(layers.LSTM(512), name = \"LSTM_layer\")(first_dense)\n",
    "\n",
    "x = layers.Dropout(0.1)(lstm_rnn)\n",
    "x = layers.Dense(264, activation = \"relu\")(x)\n",
    "simple_output = layers.Dense(len(classes), activation = \"softmax\")(x)\n",
    "\n",
    "simple_model = tf.keras.Model(inputs, simple_output)\n",
    "\n",
    "\n",
    "bi_rnn = layers.Bidirectional(layers.GRU(512), name = \"GRU_layer\")(first_dense)\n",
    "x = layers.Dropout(0.1)(bi_rnn)\n",
    "x = layers.Dense(264, activation = \"relu\")(x)\n",
    "bi_output = layers.Dense(len(classes), activation = \"softmax\")(x)\n",
    "bi_model = tf.keras.Model(inputs, bi_output)\n",
    "\n",
    "x = layers.Concatenate()([simple_model.output, bi_model.output])\n",
    "x = layers.Dense(264 , activation = \"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(264 , activation = \"relu\")(x)\n",
    "x = layers.Dense(264 , activation = \"relu\")(x)\n",
    "\n",
    "\n",
    "outputs = layers.Dense(len(classes), activation = \"softmax\")(x)\n",
    "\n",
    "model_0 = tf.keras.Model(inputs , outputs)\n",
    "\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef95f36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1/1 [==============================] - 44s 44s/step - loss: 2.2007 - accuracy: 0.1333 - val_loss: 2.1988 - val_accuracy: 0.1667\n",
      "Epoch 2/25\n",
      "1/1 [==============================] - 11s 11s/step - loss: 2.1916 - accuracy: 0.1333 - val_loss: 2.2075 - val_accuracy: 0.1667\n",
      "Epoch 3/25\n",
      "1/1 [==============================] - 11s 11s/step - loss: 2.1823 - accuracy: 0.1000 - val_loss: 2.2165 - val_accuracy: 0.1667\n",
      "Epoch 4/25\n",
      "1/1 [==============================] - 12s 12s/step - loss: 2.1763 - accuracy: 0.1333 - val_loss: 2.2263 - val_accuracy: 0.1667\n",
      "Epoch 5/25\n",
      "1/1 [==============================] - 11s 11s/step - loss: 2.1670 - accuracy: 0.2333 - val_loss: 2.2379 - val_accuracy: 0.1667\n",
      "Epoch 6/25\n",
      "1/1 [==============================] - 13s 13s/step - loss: 2.1591 - accuracy: 0.2667 - val_loss: 2.2518 - val_accuracy: 0.0833\n",
      "Epoch 7/25\n",
      "1/1 [==============================] - 12s 12s/step - loss: 2.1481 - accuracy: 0.2000 - val_loss: 2.2690 - val_accuracy: 0.0833\n",
      "Epoch 8/25\n",
      "1/1 [==============================] - 12s 12s/step - loss: 2.1386 - accuracy: 0.1667 - val_loss: 2.2900 - val_accuracy: 0.0833\n",
      "Epoch 9/25\n",
      "1/1 [==============================] - 11s 11s/step - loss: 2.1212 - accuracy: 0.1667 - val_loss: 2.3155 - val_accuracy: 0.0833\n",
      "Epoch 10/25\n",
      "1/1 [==============================] - 11s 11s/step - loss: 2.1076 - accuracy: 0.1667 - val_loss: 2.3460 - val_accuracy: 0.0833\n",
      "Epoch 11/25\n",
      "1/1 [==============================] - 11s 11s/step - loss: 2.0900 - accuracy: 0.2000 - val_loss: 2.3814 - val_accuracy: 0.0833\n",
      "Epoch 12/25\n",
      "1/1 [==============================] - 12s 12s/step - loss: 2.0717 - accuracy: 0.3000 - val_loss: 2.4224 - val_accuracy: 0.0833\n",
      "Epoch 13/25\n",
      "1/1 [==============================] - 23s 23s/step - loss: 2.0402 - accuracy: 0.3333 - val_loss: 2.4700 - val_accuracy: 0.0833\n",
      "Epoch 14/25\n",
      "1/1 [==============================] - 24s 24s/step - loss: 2.0074 - accuracy: 0.3333 - val_loss: 2.5213 - val_accuracy: 0.0833\n",
      "Epoch 15/25\n",
      "1/1 [==============================] - 23s 23s/step - loss: 1.9853 - accuracy: 0.3333 - val_loss: 2.5728 - val_accuracy: 0.0833\n",
      "Epoch 16/25\n",
      "1/1 [==============================] - 11s 11s/step - loss: 1.9401 - accuracy: 0.3333 - val_loss: 2.6190 - val_accuracy: 0.0833\n",
      "Epoch 17/25\n",
      "1/1 [==============================] - 12s 12s/step - loss: 1.8988 - accuracy: 0.3333 - val_loss: 2.6615 - val_accuracy: 0.0833\n",
      "Epoch 18/25\n",
      "1/1 [==============================] - 11s 11s/step - loss: 1.8563 - accuracy: 0.3333 - val_loss: 2.6969 - val_accuracy: 0.0833\n",
      "Epoch 19/25\n",
      "1/1 [==============================] - 12s 12s/step - loss: 1.8093 - accuracy: 0.3333 - val_loss: 2.7147 - val_accuracy: 0.0833\n",
      "Epoch 20/25\n",
      "1/1 [==============================] - 22s 22s/step - loss: 1.7602 - accuracy: 0.3333 - val_loss: 2.7296 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/25\n",
      "1/1 [==============================] - 25s 25s/step - loss: 1.6955 - accuracy: 0.3333 - val_loss: 2.7442 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/25\n",
      "1/1 [==============================] - 23s 23s/step - loss: 1.6347 - accuracy: 0.3333 - val_loss: 2.7496 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/25\n",
      "1/1 [==============================] - 21s 21s/step - loss: 1.5747 - accuracy: 0.3333 - val_loss: 2.7443 - val_accuracy: 0.1667\n",
      "Epoch 24/25\n",
      "1/1 [==============================] - 20s 20s/step - loss: 1.5093 - accuracy: 0.5667 - val_loss: 2.7387 - val_accuracy: 0.1667\n",
      "Epoch 25/25\n",
      "1/1 [==============================] - 19s 19s/step - loss: 1.4475 - accuracy: 0.6000 - val_loss: 2.7409 - val_accuracy: 0.1667\n"
     ]
    }
   ],
   "source": [
    "model_0.compile(loss = \"categorical_crossentropy\",\n",
    "               optimizer = \"adam\",\n",
    "               metrics = [\"accuracy\"])\n",
    "\n",
    "model_0_history = model_0.fit(train_datasets, \n",
    "           epochs = 25, validation_data = val_datasets )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8613770d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 9s 9s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.10132478, 0.09836511, 0.00911176, 0.1187767 , 0.1420282 ,\n",
       "        0.09863354, 0.1093633 , 0.1582581 , 0.16413857],\n",
       "       [0.10132478, 0.09836511, 0.00911176, 0.1187767 , 0.1420282 ,\n",
       "        0.09863354, 0.1093633 , 0.1582581 , 0.16413857]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask = \"hi\"\n",
    "\n",
    "pred = model_0.predict(np.array(list(ask)))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5848d9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.10132478, 0.09836511, 0.00911176, 0.1187767 , 0.1420282 ,\n",
       "        0.09863354, 0.1093633 , 0.1582581 , 0.16413857],\n",
       "       [0.10132478, 0.09836511, 0.00911176, 0.1187767 , 0.1420282 ,\n",
       "        0.09863354, 0.1093633 , 0.1582581 , 0.16413857],\n",
       "       [0.10132478, 0.09836511, 0.00911176, 0.1187767 , 0.1420282 ,\n",
       "        0.09863354, 0.1093633 , 0.1582581 , 0.16413857],\n",
       "       [0.10132478, 0.09836511, 0.00911176, 0.1187767 , 0.1420282 ,\n",
       "        0.09863354, 0.1093633 , 0.1582581 , 0.16413857],\n",
       "       [0.10132478, 0.09836511, 0.00911176, 0.1187767 , 0.1420282 ,\n",
       "        0.09863354, 0.1093633 , 0.1582581 , 0.16413857]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask = \"where\"\n",
    "\n",
    "pred = model_0.predict(np.array(list(ask)))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f1ba042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([4, 4], dtype=int64)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = tf.argmax(pred,1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d8dd56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['greating',\n",
       " 'name',\n",
       " 'health',\n",
       " 'situation',\n",
       " 'address',\n",
       " 'college',\n",
       " 'education',\n",
       " 'goodbye',\n",
       " 'location']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "261cdebfb0e5577295312261a025d85174952bea81f9e075a4efe79ca0ecd353"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
