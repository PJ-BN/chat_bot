{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc5f0c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd1328c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a14b276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intends': [{'tag': ['greating'],\n",
       "   'question': ['hello', \"what's up\", 'hey'],\n",
       "   'answer': ['Hi!']},\n",
       "  {'tag': ['name'],\n",
       "   'question': ['what is your name',\n",
       "    'nam k ho',\n",
       "    'ko ho',\n",
       "    'ko hos',\n",
       "    'who are you'],\n",
       "   'answer': ['My name is Prajwal Bhandari']},\n",
       "  {'tag': ['health'],\n",
       "   'question': ['how are you', 'sanchai xau', 'sanchai', 'thikai'],\n",
       "   'answer': [\"Yes I'm fine!\"]},\n",
       "  {'tag': ['situation'],\n",
       "   'question': ['k xa',\n",
       "    'are you okay',\n",
       "    'how do you do',\n",
       "    'are you good',\n",
       "    'how is your study going on',\n",
       "    'padai kasto xa tw'],\n",
       "   'answer': ['Thikai!']},\n",
       "  {'tag': ['address'],\n",
       "   'question': ['where do you live', 'ka basxau', 'ghar kaha ho', 'ghar'],\n",
       "   'answer': ['I live in Tilottama, Nepal']},\n",
       "  {'tag': ['college'],\n",
       "   'question': ['where do you study', 'college', 'ka padxas', 'padxas'],\n",
       "   'answer': ['Nepathya College affiliated to Tribhuvan University']},\n",
       "  {'tag': ['education'],\n",
       "   'question': ['what do you study', 'k padxau'],\n",
       "   'answer': ['Bachelor in Computer Application(BCA)']},\n",
       "  {'tag': ['goodbye'],\n",
       "   'question': ['bye',\n",
       "    'goodbye',\n",
       "    'tata',\n",
       "    'paxi bhetamla',\n",
       "    'see you later',\n",
       "    'take care'],\n",
       "   'answer': ['Bye!!']}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = open('data.json')\n",
    "json_data = json.load(d)\n",
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9892d71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "what's up\n",
      "hey\n",
      "what is your name\n",
      "nam k ho\n",
      "ko ho\n",
      "ko hos\n",
      "who are you\n",
      "how are you\n",
      "sanchai xau\n",
      "sanchai\n",
      "thikai\n",
      "k xa\n",
      "are you okay\n",
      "how do you do\n",
      "are you good\n",
      "how is your study going on\n",
      "padai kasto xa tw\n",
      "where do you live\n",
      "ka basxau\n",
      "ghar kaha ho\n",
      "ghar\n",
      "where do you study\n",
      "college\n",
      "ka padxas\n",
      "padxas\n",
      "what do you study\n",
      "k padxau\n",
      "bye\n",
      "goodbye\n",
      "tata\n",
      "paxi bhetamla\n",
      "see you later\n",
      "take care\n"
     ]
    }
   ],
   "source": [
    "ques = []\n",
    "document = []\n",
    "classes = []\n",
    "\n",
    "\n",
    "for intents in json_data['intends']:\n",
    "    for question in intents['question']:\n",
    "        print(question)\n",
    "        \n",
    "        doc = nlp(question.lower())\n",
    "        text = [token.lemma_ for token in doc]\n",
    "        \n",
    "       \n",
    "        ques.append(text)\n",
    "        document.append(( text, intents['tag']))\n",
    "\n",
    "        \n",
    "        \n",
    "    for question in intents['tag']:\n",
    "#         print(question)\n",
    "        if question not in list(classes):\n",
    "            classes.append(question)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb052c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['what', 'be', 'up'], ['greating']),\n",
       " (['what', 'do', 'you', 'study'], ['education']),\n",
       " (['take', 'care'], ['goodbye']),\n",
       " (['where', 'do', 'you', 'live'], ['address']),\n",
       " (['who', 'be', 'you'], ['name']),\n",
       " (['ko', 'hos'], ['name']),\n",
       " (['ghar'], ['address']),\n",
       " (['padxas'], ['college']),\n",
       " (['be', 'you', 'okay'], ['situation']),\n",
       " (['see', 'you', 'later'], ['goodbye']),\n",
       " (['padai', 'kasto', 'xa', 'tw'], ['situation']),\n",
       " (['paxi', 'bhetamla'], ['goodbye']),\n",
       " (['how', 'be', 'your', 'study', 'go', 'on'], ['situation']),\n",
       " (['sanchai', 'xau'], ['health']),\n",
       " (['nam', 'k', 'ho'], ['name']),\n",
       " (['ghar', 'kaha', 'ho'], ['address']),\n",
       " (['college'], ['college']),\n",
       " (['ka', 'basxau'], ['address']),\n",
       " (['hey'], ['greating']),\n",
       " (['k', 'padxau'], ['education']),\n",
       " (['tata'], ['goodbye']),\n",
       " (['hello'], ['greating']),\n",
       " (['where', 'do', 'you', 'study'], ['college']),\n",
       " (['what', 'be', 'your', 'name'], ['name']),\n",
       " (['thikai'], ['health']),\n",
       " (['bye'], ['goodbye']),\n",
       " (['how', 'do', 'you', 'do'], ['situation']),\n",
       " (['sanchai'], ['health']),\n",
       " (['k', 'xa'], ['situation']),\n",
       " (['ka', 'padxas'], ['college']),\n",
       " (['be', 'you', 'good'], ['situation']),\n",
       " (['how', 'be', 'you'], ['health']),\n",
       " (['goodbye'], ['goodbye']),\n",
       " (['ko', 'ho'], ['name'])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(document)\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcff9f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['greating',\n",
       " 'name',\n",
       " 'health',\n",
       " 'situation',\n",
       " 'address',\n",
       " 'college',\n",
       " 'education',\n",
       " 'goodbye']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3e12b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['padai kasto xa tw',\n",
       " 'paxi bhetamla',\n",
       " 'how be your study go on',\n",
       " 'sanchai xau',\n",
       " 'nam k ho',\n",
       " 'ghar kaha ho',\n",
       " 'college',\n",
       " 'ka basxau',\n",
       " 'hey',\n",
       " 'k padxau',\n",
       " 'tata',\n",
       " 'hello',\n",
       " 'where do you study',\n",
       " 'what be your name',\n",
       " 'thikai',\n",
       " 'bye',\n",
       " 'how do you do',\n",
       " 'sanchai',\n",
       " 'k xa',\n",
       " 'ka padxas',\n",
       " 'be you good',\n",
       " 'how be you',\n",
       " 'goodbye',\n",
       " 'ko ho']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques = [\" \".join(i[0]) for i in document]\n",
    "ques[10:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf778b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['greating',\n",
       " 'education',\n",
       " 'goodbye',\n",
       " 'address',\n",
       " 'name',\n",
       " 'name',\n",
       " 'address',\n",
       " 'college',\n",
       " 'situation',\n",
       " 'goodbye',\n",
       " 'situation',\n",
       " 'goodbye',\n",
       " 'situation',\n",
       " 'health',\n",
       " 'name',\n",
       " 'address',\n",
       " 'college',\n",
       " 'address',\n",
       " 'greating',\n",
       " 'education',\n",
       " 'goodbye',\n",
       " 'greating',\n",
       " 'college',\n",
       " 'name',\n",
       " 'health',\n",
       " 'goodbye',\n",
       " 'situation',\n",
       " 'health',\n",
       " 'situation',\n",
       " 'college',\n",
       " 'situation',\n",
       " 'health',\n",
       " 'goodbye',\n",
       " 'name']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = [\"\".join(i[1]) for i in document]\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f48fcb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Documents\\ML\\NLP\\bot\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "\n",
    "train_one_hot = ohe.fit_transform(np.array(ans).reshape(-1,1))\n",
    "train_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5dc0917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 8), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets = tf.data.Dataset.from_tensor_slices((ques, train_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75992ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 1000\n",
    "output_length = 100\n",
    "\n",
    "text_vector = layers.TextVectorization(max_tokens = max_vocab+1 , output_sequence_length=output_length )\n",
    "\n",
    "text_embed = layers.Embedding(max_vocab, 128, mask_zero=True)\n",
    "\n",
    "text_vector.adapt(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dea4225b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 100)              0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 100, 128)          128000    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100, 500)          64500     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100, 500)          0         \n",
      "                                                                 \n",
      " Bidirectional_layer (Bidire  (None, 1000)             4004000   \n",
      " ctional)                                                        \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 4008      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,701,008\n",
      "Trainable params: 4,701,008\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(shape = (1,), dtype = tf.string)\n",
    "vector = text_vector(inputs)\n",
    "embed = text_embed(vector)\n",
    "x = layers.Dense(500, activation = \"relu\")(embed)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Bidirectional(layers.LSTM(500), name = \"Bidirectional_layer\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "\n",
    "x = layers.Dense(500 , activation = \"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "\n",
    "\n",
    "outputs = layers.Dense(len(classes), activation = \"softmax\")(x)\n",
    "\n",
    "model_0 = tf.keras.Model(inputs , outputs)\n",
    "\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef95f36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "2/2 [==============================] - 29s 1s/step - loss: 2.0798 - accuracy: 0.0000e+00\n",
      "Epoch 2/25\n",
      "2/2 [==============================] - 5s 1s/step - loss: 2.0564 - accuracy: 0.3529\n",
      "Epoch 3/25\n",
      "2/2 [==============================] - 5s 2s/step - loss: 2.0277 - accuracy: 0.2059\n",
      "Epoch 4/25\n",
      "2/2 [==============================] - 7s 2s/step - loss: 1.9835 - accuracy: 0.2059\n",
      "Epoch 5/25\n",
      "2/2 [==============================] - 13s 3s/step - loss: 1.9268 - accuracy: 0.1765\n",
      "Epoch 6/25\n",
      "2/2 [==============================] - 15s 3s/step - loss: 1.8998 - accuracy: 0.2059\n",
      "Epoch 7/25\n",
      "2/2 [==============================] - 11s 2s/step - loss: 1.8822 - accuracy: 0.2059\n",
      "Epoch 8/25\n",
      "2/2 [==============================] - 9s 1s/step - loss: 1.7952 - accuracy: 0.2941\n",
      "Epoch 9/25\n",
      "2/2 [==============================] - 9s 3s/step - loss: 1.6453 - accuracy: 0.4706\n",
      "Epoch 10/25\n",
      "2/2 [==============================] - 10s 1s/step - loss: 1.5048 - accuracy: 0.4118\n",
      "Epoch 11/25\n",
      "2/2 [==============================] - 7s 2s/step - loss: 1.3965 - accuracy: 0.4118\n",
      "Epoch 12/25\n",
      "2/2 [==============================] - 9s 2s/step - loss: 1.2721 - accuracy: 0.4412\n",
      "Epoch 13/25\n",
      "2/2 [==============================] - 8s 1s/step - loss: 1.1831 - accuracy: 0.5294\n",
      "Epoch 14/25\n",
      "2/2 [==============================] - 5s 1s/step - loss: 1.0922 - accuracy: 0.6176\n",
      "Epoch 15/25\n",
      "2/2 [==============================] - 5s 1s/step - loss: 1.0133 - accuracy: 0.6471\n",
      "Epoch 16/25\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.9422 - accuracy: 0.6765\n",
      "Epoch 17/25\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.8622 - accuracy: 0.7647\n",
      "Epoch 18/25\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.8187 - accuracy: 0.7647\n",
      "Epoch 19/25\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.6909 - accuracy: 0.9118\n",
      "Epoch 20/25\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.5773 - accuracy: 0.9118\n",
      "Epoch 21/25\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.4838 - accuracy: 0.9706\n",
      "Epoch 22/25\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.3817 - accuracy: 0.9706\n",
      "Epoch 23/25\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.2854 - accuracy: 1.0000\n",
      "Epoch 24/25\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.1816 - accuracy: 1.0000\n",
      "Epoch 25/25\n",
      "2/2 [==============================] - 9s 3s/step - loss: 0.1294 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model_0.compile(loss = \"categorical_crossentropy\",\n",
    "               optimizer = \"adam\",\n",
    "               metrics = [\"accuracy\"])\n",
    "\n",
    "model_0_history = model_0.fit(train_datasets, \n",
    "           epochs = 25 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8613770d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 354ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.11045629, 0.03133829, 0.025288  , 0.24579218, 0.21640883,\n",
       "        0.08141471, 0.21370453, 0.07559713],\n",
       "       [0.11045629, 0.03133829, 0.025288  , 0.24579218, 0.21640883,\n",
       "        0.08141471, 0.21370453, 0.07559713],\n",
       "       [0.11045629, 0.03133829, 0.025288  , 0.24579218, 0.21640883,\n",
       "        0.08141471, 0.21370453, 0.07559713],\n",
       "       [0.11045629, 0.03133829, 0.025288  , 0.24579218, 0.21640883,\n",
       "        0.08141471, 0.21370453, 0.07559713],\n",
       "       [0.11045629, 0.03133829, 0.025288  , 0.24579218, 0.21640883,\n",
       "        0.08141471, 0.21370453, 0.07559713]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask = \"hello\"\n",
    "\n",
    "pred = model_0.predict(np.array(list(ask)))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f1ba042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int64, numpy=array([3, 3, 3], dtype=int64)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = tf.argmax(pred,1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d8dd56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['greating',\n",
       " 'name',\n",
       " 'health',\n",
       " 'situation',\n",
       " 'address',\n",
       " 'college',\n",
       " 'education',\n",
       " 'goodbye']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f67e08f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "261cdebfb0e5577295312261a025d85174952bea81f9e075a4efe79ca0ecd353"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
