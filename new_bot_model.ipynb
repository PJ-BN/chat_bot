{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a **AI** based chat bot trained in a small dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all the required model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intends': [{'tag': ['greeting'],\n",
       "   'question': ['hello', \"what's up\", 'hey', 'hi', 'hey there', 'greeting'],\n",
       "   'answer': ['Hi!']},\n",
       "  {'tag': ['name'],\n",
       "   'question': ['what is your name',\n",
       "    'introduce',\n",
       "    'introduce yourself',\n",
       "    'who',\n",
       "    'name',\n",
       "    'who are you',\n",
       "    'may i have your name',\n",
       "    'would you mind telling me your name',\n",
       "    'how can i call you'],\n",
       "   'answer': ['My name is Prajwal Bhandari']},\n",
       "  {'tag': ['health'],\n",
       "   'question': ['how are you',\n",
       "    'fine',\n",
       "    'how is your health',\n",
       "    'health',\n",
       "    'how are you feeling',\n",
       "    'how are you doing now',\n",
       "    'how is yours health been',\n",
       "    'is everything okay about your health'],\n",
       "   'answer': [\"Yes I'm fine!\"]},\n",
       "  {'tag': ['situation'],\n",
       "   'question': ['how is it going',\n",
       "    'are you okay',\n",
       "    'how do you do',\n",
       "    'are you good',\n",
       "    'how is your study going on'],\n",
       "   'answer': ['Thikai!']},\n",
       "  {'tag': ['address'],\n",
       "   'question': ['where do you live',\n",
       "    'home',\n",
       "    'where is your home',\n",
       "    'what is your address',\n",
       "    'address',\n",
       "    'could you please give me your address',\n",
       "    'can you tell me your address',\n",
       "    'may i have your address'],\n",
       "   'answer': ['I live in Tilottama, Nepal']},\n",
       "  {'tag': ['college'],\n",
       "   'question': ['where do you study',\n",
       "    'college',\n",
       "    'which university do you study',\n",
       "    'university'],\n",
       "   'answer': ['Nepathya College affiliated to Tribhuvan University']},\n",
       "  {'tag': ['education'],\n",
       "   'question': ['what do you study',\n",
       "    'subject',\n",
       "    'what is your major',\n",
       "    'what field did you specialize during your education',\n",
       "    'what is your educational background'],\n",
       "   'answer': ['Bachelor in Computer Application(BCA)']},\n",
       "  {'tag': ['location'],\n",
       "   'question': ['your current location',\n",
       "    'where',\n",
       "    'location',\n",
       "    'where are you',\n",
       "    'where are you located',\n",
       "    'can you tell where you are currently based on',\n",
       "    'may i ask you where you are texting from'],\n",
       "   'answer': ['Ghar']},\n",
       "  {'tag': ['goodbye'],\n",
       "   'question': ['bye',\n",
       "    'goodbye',\n",
       "    'later',\n",
       "    'see you next time',\n",
       "    'see you later',\n",
       "    'take care'],\n",
       "   'answer': ['Bye!!']}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "d = open('data.json')\n",
    "json_data = json.load(d)\n",
    "json_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmanizing and Tokenizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "documents = []\n",
    "classes = []\n",
    "\n",
    "\n",
    "\n",
    "for intents in json_data['intends']:\n",
    "    for question in intents['question']:\n",
    "        \n",
    "        \n",
    "        doc = nlp(question.lower())\n",
    "        text = [token.lemma_ for token in doc]\n",
    "        \n",
    "        \n",
    "       \n",
    "        words.extend(text)\n",
    "        \n",
    "        documents.append((text, intents[\"tag\"]))\n",
    "        \n",
    "        if intents[\"tag\"] not in list(classes):\n",
    "            classes.append(intents[\"tag\"])\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'about',\n",
       " 'address',\n",
       " 'ask',\n",
       " 'background',\n",
       " 'base',\n",
       " 'be',\n",
       " 'bye',\n",
       " 'call',\n",
       " 'can',\n",
       " 'care',\n",
       " 'college',\n",
       " 'could',\n",
       " 'current',\n",
       " 'currently',\n",
       " 'do',\n",
       " 'during',\n",
       " 'education',\n",
       " 'educational',\n",
       " 'everything',\n",
       " 'feel',\n",
       " 'field',\n",
       " 'fine',\n",
       " 'from',\n",
       " 'give',\n",
       " 'go',\n",
       " 'good',\n",
       " 'goodbye',\n",
       " 'greet',\n",
       " 'have',\n",
       " 'health',\n",
       " 'hello',\n",
       " 'hey',\n",
       " 'hi',\n",
       " 'home',\n",
       " 'how',\n",
       " 'introduce',\n",
       " 'it',\n",
       " 'later',\n",
       " 'live',\n",
       " 'locate',\n",
       " 'location',\n",
       " 'major',\n",
       " 'may',\n",
       " 'mind',\n",
       " 'name',\n",
       " 'next',\n",
       " 'now',\n",
       " 'okay',\n",
       " 'on',\n",
       " 'please',\n",
       " 'see',\n",
       " 'specialize',\n",
       " 'study',\n",
       " 'subject',\n",
       " 'take',\n",
       " 'tell',\n",
       " 'texte',\n",
       " 'there',\n",
       " 'time',\n",
       " 'university',\n",
       " 'up',\n",
       " 'what',\n",
       " 'where',\n",
       " 'which',\n",
       " 'who',\n",
       " 'would',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = sorted(set(words))\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['hello'], ['greeting']),\n",
       " (['what', 'be', 'up'], ['greeting']),\n",
       " (['hey'], ['greeting']),\n",
       " (['hi'], ['greeting']),\n",
       " (['hey', 'there'], ['greeting']),\n",
       " (['greet'], ['greeting']),\n",
       " (['what', 'be', 'your', 'name'], ['name']),\n",
       " (['introduce'], ['name']),\n",
       " (['introduce', 'yourself'], ['name']),\n",
       " (['who'], ['name']),\n",
       " (['name'], ['name']),\n",
       " (['who', 'be', 'you'], ['name']),\n",
       " (['may', 'I', 'have', 'your', 'name'], ['name']),\n",
       " (['would', 'you', 'mind', 'tell', 'I', 'your', 'name'], ['name']),\n",
       " (['how', 'can', 'I', 'call', 'you'], ['name']),\n",
       " (['how', 'be', 'you'], ['health']),\n",
       " (['fine'], ['health']),\n",
       " (['how', 'be', 'your', 'health'], ['health']),\n",
       " (['health'], ['health']),\n",
       " (['how', 'be', 'you', 'feel'], ['health']),\n",
       " (['how', 'be', 'you', 'do', 'now'], ['health']),\n",
       " (['how', 'be', 'yours', 'health', 'be'], ['health']),\n",
       " (['be', 'everything', 'okay', 'about', 'your', 'health'], ['health']),\n",
       " (['how', 'be', 'it', 'go'], ['situation']),\n",
       " (['be', 'you', 'okay'], ['situation']),\n",
       " (['how', 'do', 'you', 'do'], ['situation']),\n",
       " (['be', 'you', 'good'], ['situation']),\n",
       " (['how', 'be', 'your', 'study', 'go', 'on'], ['situation']),\n",
       " (['where', 'do', 'you', 'live'], ['address']),\n",
       " (['home'], ['address']),\n",
       " (['where', 'be', 'your', 'home'], ['address']),\n",
       " (['what', 'be', 'your', 'address'], ['address']),\n",
       " (['address'], ['address']),\n",
       " (['could', 'you', 'please', 'give', 'I', 'your', 'address'], ['address']),\n",
       " (['can', 'you', 'tell', 'I', 'your', 'address'], ['address']),\n",
       " (['may', 'I', 'have', 'your', 'address'], ['address']),\n",
       " (['where', 'do', 'you', 'study'], ['college']),\n",
       " (['college'], ['college']),\n",
       " (['which', 'university', 'do', 'you', 'study'], ['college']),\n",
       " (['university'], ['college']),\n",
       " (['what', 'do', 'you', 'study'], ['education']),\n",
       " (['subject'], ['education']),\n",
       " (['what', 'be', 'your', 'major'], ['education']),\n",
       " (['what', 'field', 'do', 'you', 'specialize', 'during', 'your', 'education'],\n",
       "  ['education']),\n",
       " (['what', 'be', 'your', 'educational', 'background'], ['education']),\n",
       " (['your', 'current', 'location'], ['location']),\n",
       " (['where'], ['location']),\n",
       " (['location'], ['location']),\n",
       " (['where', 'be', 'you'], ['location']),\n",
       " (['where', 'be', 'you', 'locate'], ['location']),\n",
       " (['can', 'you', 'tell', 'where', 'you', 'be', 'currently', 'base', 'on'],\n",
       "  ['location']),\n",
       " (['may', 'I', 'ask', 'you', 'where', 'you', 'be', 'texte', 'from'],\n",
       "  ['location']),\n",
       " (['bye'], ['goodbye']),\n",
       " (['goodbye'], ['goodbye']),\n",
       " (['later'], ['goodbye']),\n",
       " (['see', 'you', 'next', 'time'], ['goodbye']),\n",
       " (['see', 'you', 'later'], ['goodbye']),\n",
       " (['take', 'care'], ['goodbye'])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing string data in numeric format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data =[]\n",
    "output_empty = [0] * len(classes)\n",
    "for document in documents:\n",
    "    bag = []\n",
    "   \n",
    "    \n",
    "    word_pattern = document[0]\n",
    "            \n",
    "    \n",
    "    for word in words:\n",
    "        bag.append(1) if word in word_pattern else bag.append(0)\n",
    "\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(document[1])] = 1\n",
    "    training_data.append([bag, output_row]) \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(training_data )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5224\\4066987790.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training_data = np.array(training_data)\n"
     ]
    }
   ],
   "source": [
    "training_data = np.array(training_data)\n",
    "\n",
    "train_x = list(training_data[:,0])\n",
    "train_y = list(training_data[:,1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 71)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 264)               19008     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 264)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 264)               69960     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 264)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 264)               69960     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 264)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 264)               69960     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 264)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 264)               69960     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 9)                 2385      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 301,233\n",
      "Trainable params: 301,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(shape = (len(train_x[0]),), dtype = tf.int32)\n",
    "\n",
    "x = layers.Dense(264, activation = \"relu\")(inputs)\n",
    "# lstm_rnn = layers.Bidirectional(layers.LSTM(512), name = \"LSTM_layer\")(first_dense)\n",
    "\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(264, activation = \"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(264, activation = \"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(264, activation = \"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(264, activation = \"relu\")(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "outputs = layers.Dense(len(classes), activation = \"softmax\")(x)\n",
    "\n",
    "model_0 = tf.keras.Model(inputs , outputs)\n",
    "\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - 2s 16ms/step - loss: 2.1891 - accuracy: 0.1379\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.2008 - accuracy: 0.0862\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.1620 - accuracy: 0.1552\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.1663 - accuracy: 0.1207\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.1591 - accuracy: 0.1034\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.1584 - accuracy: 0.1724\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.1347 - accuracy: 0.1724\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.1430 - accuracy: 0.2241\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.1253 - accuracy: 0.1034\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.1349 - accuracy: 0.1379\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.1264 - accuracy: 0.1552\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.0706 - accuracy: 0.1724\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.0683 - accuracy: 0.1379\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.0939 - accuracy: 0.2414\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.0694 - accuracy: 0.2069\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.0722 - accuracy: 0.2414\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.9924 - accuracy: 0.2586\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.0498 - accuracy: 0.2586\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.0124 - accuracy: 0.3103\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.0283 - accuracy: 0.2759\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.9507 - accuracy: 0.2931\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.9297 - accuracy: 0.2241\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.9413 - accuracy: 0.2931\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.8940 - accuracy: 0.3793\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.8834 - accuracy: 0.3276\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.8249 - accuracy: 0.3793\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.8721 - accuracy: 0.3276\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.7159 - accuracy: 0.3966\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.6732 - accuracy: 0.3448\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.7170 - accuracy: 0.3793\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.6149 - accuracy: 0.4310\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.5123 - accuracy: 0.4655\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.4919 - accuracy: 0.3966\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.4454 - accuracy: 0.4828\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.4541 - accuracy: 0.5000\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.2287 - accuracy: 0.5862\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.4011 - accuracy: 0.4483\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.2725 - accuracy: 0.5862\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.1519 - accuracy: 0.6552\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.0767 - accuracy: 0.6207\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.1225 - accuracy: 0.6207\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.8960 - accuracy: 0.7414\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.8187 - accuracy: 0.7414\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.0570 - accuracy: 0.6034\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.0195 - accuracy: 0.6379\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.8260 - accuracy: 0.7069\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.7128 - accuracy: 0.7586\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.7955 - accuracy: 0.7069\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.0224 - accuracy: 0.6552\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.7039 - accuracy: 0.7586\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5461 - accuracy: 0.8448\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.5565 - accuracy: 0.7759\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4241 - accuracy: 0.8103\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3773 - accuracy: 0.8966\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4996 - accuracy: 0.8276\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5153 - accuracy: 0.8103\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4205 - accuracy: 0.8276\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.4544 - accuracy: 0.8103\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.4564 - accuracy: 0.8448\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.4971 - accuracy: 0.8793\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.4407 - accuracy: 0.8448\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.4387 - accuracy: 0.8966\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3241 - accuracy: 0.9138\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4769 - accuracy: 0.7931\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.2341 - accuracy: 0.9310\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.3502 - accuracy: 0.8793\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.1610 - accuracy: 0.9655\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.2442 - accuracy: 0.8966\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.3038 - accuracy: 0.8621\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1828 - accuracy: 0.8966\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1676 - accuracy: 0.9310\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.2658 - accuracy: 0.9138\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.1391 - accuracy: 0.9655\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2453 - accuracy: 0.8966\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1816 - accuracy: 0.9828\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2927 - accuracy: 0.8621\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2148 - accuracy: 0.8793\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.1661 - accuracy: 0.9310\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.1683 - accuracy: 0.9483\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.1981 - accuracy: 0.9310\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.2253 - accuracy: 0.8966\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.1274 - accuracy: 0.9655\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.2013 - accuracy: 0.9138\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.1817 - accuracy: 0.9310\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.1007 - accuracy: 0.9828\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1019 - accuracy: 0.9483\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.2514 - accuracy: 0.9310\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.1440 - accuracy: 0.9310\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.1438 - accuracy: 0.9483\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1151 - accuracy: 0.9483\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1126 - accuracy: 0.9828\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1084 - accuracy: 0.9828\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1528 - accuracy: 0.9655\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0919 - accuracy: 0.9655\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.1856 - accuracy: 0.9138\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.1140 - accuracy: 0.9655\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0662 - accuracy: 0.9828\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1020 - accuracy: 0.9310\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.1153 - accuracy: 0.9655\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0509 - accuracy: 0.9828\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0555 - accuracy: 0.9828\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.1441 - accuracy: 0.9655\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0899 - accuracy: 0.9655\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0292 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0966 - accuracy: 0.9828\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0268 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.1275 - accuracy: 0.9655\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0820 - accuracy: 0.9655\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0701 - accuracy: 0.9828\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0564 - accuracy: 0.9828\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0457 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0695 - accuracy: 0.9655\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0412 - accuracy: 0.9828\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0776 - accuracy: 0.9655\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0467 - accuracy: 0.9828\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1253 - accuracy: 0.9483\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0923 - accuracy: 0.9655\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1342 - accuracy: 0.9655\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0700 - accuracy: 0.9655\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0605 - accuracy: 0.9655\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1109 - accuracy: 0.9655\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0396 - accuracy: 0.9828\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0429 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0902 - accuracy: 0.9483\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0964 - accuracy: 0.9483\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.1580 - accuracy: 0.9828\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0456 - accuracy: 0.9828\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0240 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0303 - accuracy: 0.9828\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0460 - accuracy: 0.9655\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.1398 - accuracy: 0.9483\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0468 - accuracy: 0.9828\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0579 - accuracy: 0.9828\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0246 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0672 - accuracy: 0.9828\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0420 - accuracy: 0.9828\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.1638 - accuracy: 0.9310\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0304 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0621 - accuracy: 0.9655\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0166 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0326 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0945 - accuracy: 0.9828\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0316 - accuracy: 0.9828\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0246 - accuracy: 0.9828\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0382 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0268 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0759 - accuracy: 0.9828\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0390 - accuracy: 0.9828\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0255 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0817 - accuracy: 0.9655\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0360 - accuracy: 0.9828\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0706 - accuracy: 0.9655\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0291 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0638 - accuracy: 0.9828\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0262 - accuracy: 0.9828\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0423 - accuracy: 0.9828\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0427 - accuracy: 0.9828\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0306 - accuracy: 0.9828\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0487 - accuracy: 0.9828\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0660 - accuracy: 0.9655\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0830 - accuracy: 0.9828\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.2311 - accuracy: 0.9655\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0203 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0165 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0313 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0676 - accuracy: 0.9828\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0266 - accuracy: 0.9828\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0421 - accuracy: 0.9828\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0255 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0205 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0460 - accuracy: 0.9828\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0378 - accuracy: 0.9828\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0289 - accuracy: 0.9828\n",
      "Epoch 180/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0189 - accuracy: 0.9828\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0836 - accuracy: 0.9828\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0421 - accuracy: 0.9828\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1801 - accuracy: 0.9655\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0229 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1889 - accuracy: 0.9655\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0585 - accuracy: 0.9655\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0427 - accuracy: 0.9828\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1181 - accuracy: 0.9655\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0150 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0241 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0677 - accuracy: 0.9828\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0399 - accuracy: 0.9828\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0176 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model_0.compile(loss = \"categorical_crossentropy\",\n",
    "               optimizer = \"adam\",\n",
    "               metrics = [\"accuracy\"])\n",
    "\n",
    "model_0_history = model_0.fit(x = np.array(train_x), y = np.array(train_y), \n",
    "           epochs = 200 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = \"where do you study\"\n",
    "\n",
    "val_in_num = [0] * len(words)\n",
    "for i ,word in enumerate(words):\n",
    "        if word in val:\n",
    "            val_in_num[i] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 144ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.6179288e-07, 1.2135545e-08, 2.1360047e-09, 1.2003484e-05,\n",
       "        1.1935554e-05, 9.9978262e-01, 5.0141603e-05, 7.4370830e-05,\n",
       "        6.8114517e-05]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model_0.predict(np.array([val_in_num]))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int64, numpy=array([5], dtype=int64)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = tf.argmax(pred ,1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['greeting'],\n",
       " ['name'],\n",
       " ['health'],\n",
       " ['situation'],\n",
       " ['address'],\n",
       " ['college'],\n",
       " ['education'],\n",
       " ['location'],\n",
       " ['goodbye']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model and some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(words, open(\"words.p\", \"wb\"))\n",
    "pickle.dump(classes, open(\"classes.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
