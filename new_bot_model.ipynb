{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intends': [{'tag': ['greeting'],\n",
       "   'question': ['hello', \"what's up\", 'hey', 'hi', 'hey there', 'greeting'],\n",
       "   'answer': ['Hi!']},\n",
       "  {'tag': ['name'],\n",
       "   'question': ['what is your name',\n",
       "    'introduce',\n",
       "    'introduce yourself',\n",
       "    'who',\n",
       "    'name',\n",
       "    'who are you',\n",
       "    'may i have your name',\n",
       "    'would you mind telling me your name',\n",
       "    'how can i call you'],\n",
       "   'answer': ['My name is Prajwal Bhandari']},\n",
       "  {'tag': ['health'],\n",
       "   'question': ['how are you',\n",
       "    'fine',\n",
       "    'how is your health',\n",
       "    'health',\n",
       "    'how are you feeling',\n",
       "    'how are you doing now',\n",
       "    'how is yours health been',\n",
       "    'is everything okay about your health'],\n",
       "   'answer': [\"Yes I'm fine!\"]},\n",
       "  {'tag': ['situation'],\n",
       "   'question': ['how is it going',\n",
       "    'are you okay',\n",
       "    'how do you do',\n",
       "    'are you good',\n",
       "    'how is your study going on'],\n",
       "   'answer': ['Thikai!']},\n",
       "  {'tag': ['address'],\n",
       "   'question': ['where do you live',\n",
       "    'home',\n",
       "    'where is your home',\n",
       "    'what is your address',\n",
       "    'address',\n",
       "    'could you please give me your address',\n",
       "    'can you tell me your address',\n",
       "    'may i have your address'],\n",
       "   'answer': ['I live in Tilottama, Nepal']},\n",
       "  {'tag': ['college'],\n",
       "   'question': ['where do you study',\n",
       "    'college',\n",
       "    'which university do you study',\n",
       "    'university'],\n",
       "   'answer': ['Nepathya College affiliated to Tribhuvan University']},\n",
       "  {'tag': ['education'],\n",
       "   'question': ['what do you study',\n",
       "    'subject',\n",
       "    'what is your major',\n",
       "    'what field did you specialize during your education',\n",
       "    'what is your educational background'],\n",
       "   'answer': ['Bachelor in Computer Application(BCA)']},\n",
       "  {'tag': ['goodbye'],\n",
       "   'question': ['bye',\n",
       "    'goodbye',\n",
       "    'later',\n",
       "    'see you next time',\n",
       "    'see you later',\n",
       "    'take care'],\n",
       "   'answer': ['Bye!!']},\n",
       "  {'tag': ['location'],\n",
       "   'question': ['your current location',\n",
       "    'where',\n",
       "    'location',\n",
       "    'where are you',\n",
       "    'where are you located',\n",
       "    'can you tell where you are currently based on',\n",
       "    'may i ask you where you are texting from'],\n",
       "   'answer': ['Ghar']}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "d = open('data.json')\n",
    "json_data = json.load(d)\n",
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "documents = []\n",
    "classes = []\n",
    "\n",
    "\n",
    "\n",
    "for intents in json_data['intends']:\n",
    "    for question in intents['question']:\n",
    "        \n",
    "        \n",
    "        doc = nlp(question.lower())\n",
    "        text = [token.lemma_ for token in doc]\n",
    "        \n",
    "        \n",
    "       \n",
    "        words.extend(text)\n",
    "        \n",
    "        documents.append((text, intents[\"tag\"]))\n",
    "        \n",
    "        if intents[\"tag\"] not in list(classes):\n",
    "            classes.append(intents[\"tag\"])\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'about',\n",
       " 'address',\n",
       " 'ask',\n",
       " 'background',\n",
       " 'base',\n",
       " 'be',\n",
       " 'bye',\n",
       " 'call',\n",
       " 'can',\n",
       " 'care',\n",
       " 'college',\n",
       " 'could',\n",
       " 'current',\n",
       " 'currently',\n",
       " 'do',\n",
       " 'during',\n",
       " 'education',\n",
       " 'educational',\n",
       " 'everything',\n",
       " 'feel',\n",
       " 'field',\n",
       " 'fine',\n",
       " 'from',\n",
       " 'give',\n",
       " 'go',\n",
       " 'good',\n",
       " 'goodbye',\n",
       " 'greet',\n",
       " 'have',\n",
       " 'health',\n",
       " 'hello',\n",
       " 'hey',\n",
       " 'hi',\n",
       " 'home',\n",
       " 'how',\n",
       " 'introduce',\n",
       " 'it',\n",
       " 'later',\n",
       " 'live',\n",
       " 'locate',\n",
       " 'location',\n",
       " 'major',\n",
       " 'may',\n",
       " 'mind',\n",
       " 'name',\n",
       " 'next',\n",
       " 'now',\n",
       " 'okay',\n",
       " 'on',\n",
       " 'please',\n",
       " 'see',\n",
       " 'specialize',\n",
       " 'study',\n",
       " 'subject',\n",
       " 'take',\n",
       " 'tell',\n",
       " 'texte',\n",
       " 'there',\n",
       " 'time',\n",
       " 'university',\n",
       " 'up',\n",
       " 'what',\n",
       " 'where',\n",
       " 'which',\n",
       " 'who',\n",
       " 'would',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = sorted(set(words))\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['hello'], ['greeting']),\n",
       " (['what', 'be', 'up'], ['greeting']),\n",
       " (['hey'], ['greeting']),\n",
       " (['hi'], ['greeting']),\n",
       " (['hey', 'there'], ['greeting']),\n",
       " (['greet'], ['greeting']),\n",
       " (['what', 'be', 'your', 'name'], ['name']),\n",
       " (['introduce'], ['name']),\n",
       " (['introduce', 'yourself'], ['name']),\n",
       " (['who'], ['name']),\n",
       " (['name'], ['name']),\n",
       " (['who', 'be', 'you'], ['name']),\n",
       " (['may', 'I', 'have', 'your', 'name'], ['name']),\n",
       " (['would', 'you', 'mind', 'tell', 'I', 'your', 'name'], ['name']),\n",
       " (['how', 'can', 'I', 'call', 'you'], ['name']),\n",
       " (['how', 'be', 'you'], ['health']),\n",
       " (['fine'], ['health']),\n",
       " (['how', 'be', 'your', 'health'], ['health']),\n",
       " (['health'], ['health']),\n",
       " (['how', 'be', 'you', 'feel'], ['health']),\n",
       " (['how', 'be', 'you', 'do', 'now'], ['health']),\n",
       " (['how', 'be', 'yours', 'health', 'be'], ['health']),\n",
       " (['be', 'everything', 'okay', 'about', 'your', 'health'], ['health']),\n",
       " (['how', 'be', 'it', 'go'], ['situation']),\n",
       " (['be', 'you', 'okay'], ['situation']),\n",
       " (['how', 'do', 'you', 'do'], ['situation']),\n",
       " (['be', 'you', 'good'], ['situation']),\n",
       " (['how', 'be', 'your', 'study', 'go', 'on'], ['situation']),\n",
       " (['where', 'do', 'you', 'live'], ['address']),\n",
       " (['home'], ['address']),\n",
       " (['where', 'be', 'your', 'home'], ['address']),\n",
       " (['what', 'be', 'your', 'address'], ['address']),\n",
       " (['address'], ['address']),\n",
       " (['could', 'you', 'please', 'give', 'I', 'your', 'address'], ['address']),\n",
       " (['can', 'you', 'tell', 'I', 'your', 'address'], ['address']),\n",
       " (['may', 'I', 'have', 'your', 'address'], ['address']),\n",
       " (['where', 'do', 'you', 'study'], ['college']),\n",
       " (['college'], ['college']),\n",
       " (['which', 'university', 'do', 'you', 'study'], ['college']),\n",
       " (['university'], ['college']),\n",
       " (['what', 'do', 'you', 'study'], ['education']),\n",
       " (['subject'], ['education']),\n",
       " (['what', 'be', 'your', 'major'], ['education']),\n",
       " (['what', 'field', 'do', 'you', 'specialize', 'during', 'your', 'education'],\n",
       "  ['education']),\n",
       " (['what', 'be', 'your', 'educational', 'background'], ['education']),\n",
       " (['bye'], ['goodbye']),\n",
       " (['goodbye'], ['goodbye']),\n",
       " (['later'], ['goodbye']),\n",
       " (['see', 'you', 'next', 'time'], ['goodbye']),\n",
       " (['see', 'you', 'later'], ['goodbye']),\n",
       " (['take', 'care'], ['goodbye']),\n",
       " (['your', 'current', 'location'], ['location']),\n",
       " (['where'], ['location']),\n",
       " (['location'], ['location']),\n",
       " (['where', 'be', 'you'], ['location']),\n",
       " (['where', 'be', 'you', 'locate'], ['location']),\n",
       " (['can', 'you', 'tell', 'where', 'you', 'be', 'currently', 'base', 'on'],\n",
       "  ['location']),\n",
       " (['may', 'I', 'ask', 'you', 'where', 'you', 'be', 'texte', 'from'],\n",
       "  ['location'])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data =[]\n",
    "output_empty = [0] * len(classes)\n",
    "for document in documents:\n",
    "    bag = []\n",
    "   \n",
    "    \n",
    "    word_pattern = document[0]\n",
    "            \n",
    "    \n",
    "    for word in words:\n",
    "        bag.append(1) if word in word_pattern else bag.append(0)\n",
    "\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(document[1])] = 1\n",
    "    training_data.append([bag, output_row]) \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(training_data )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13572\\4066987790.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training_data = np.array(training_data)\n"
     ]
    }
   ],
   "source": [
    "training_data = np.array(training_data)\n",
    "\n",
    "train_x = list(training_data[:,0])\n",
    "train_y = list(training_data[:,1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 71)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 264)               19008     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 264)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 264)               69960     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 264)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 264)               69960     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 264)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 264)               69960     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 264)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 264)               69960     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 9)                 2385      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 301,233\n",
      "Trainable params: 301,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(shape = (len(train_x[0]),), dtype = tf.int32)\n",
    "\n",
    "x = layers.Dense(264, activation = \"relu\")(inputs)\n",
    "# lstm_rnn = layers.Bidirectional(layers.LSTM(512), name = \"LSTM_layer\")(first_dense)\n",
    "\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(264, activation = \"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(264, activation = \"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(264, activation = \"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(264, activation = \"relu\")(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "outputs = layers.Dense(len(classes), activation = \"softmax\")(x)\n",
    "\n",
    "model_0 = tf.keras.Model(inputs , outputs)\n",
    "\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - 2s 16ms/step - loss: 2.2278 - accuracy: 0.1552\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.1981 - accuracy: 0.1379\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.1869 - accuracy: 0.2069\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.1686 - accuracy: 0.1552\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.1541 - accuracy: 0.1897\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.1630 - accuracy: 0.1724\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 2.1493 - accuracy: 0.1724\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.1412 - accuracy: 0.1379\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.1421 - accuracy: 0.1724\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.1248 - accuracy: 0.1552\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 2.1252 - accuracy: 0.0862\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 2.0937 - accuracy: 0.1897\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 2.0845 - accuracy: 0.2069\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 2.0837 - accuracy: 0.1724\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.0880 - accuracy: 0.2414\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.0657 - accuracy: 0.1724\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.0926 - accuracy: 0.2069\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 2.0046 - accuracy: 0.2586\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.0672 - accuracy: 0.1724\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 2.0400 - accuracy: 0.1897\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.0539 - accuracy: 0.2586\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.9628 - accuracy: 0.3276\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.9201 - accuracy: 0.2931\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.0088 - accuracy: 0.2069\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.9486 - accuracy: 0.2759\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.8698 - accuracy: 0.3103\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.8446 - accuracy: 0.3448\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.8094 - accuracy: 0.3448\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.8362 - accuracy: 0.3276\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.7668 - accuracy: 0.3276\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.7049 - accuracy: 0.3793\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.6829 - accuracy: 0.3448\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.5597 - accuracy: 0.4483\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 1.5396 - accuracy: 0.4138\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.4958 - accuracy: 0.4655\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 1.4993 - accuracy: 0.4138\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.4002 - accuracy: 0.5000\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.4140 - accuracy: 0.5172\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.2536 - accuracy: 0.5690\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.3255 - accuracy: 0.5517\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.1833 - accuracy: 0.5690\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.1262 - accuracy: 0.6724\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.0178 - accuracy: 0.6207\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.0032 - accuracy: 0.6379\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.9139 - accuracy: 0.6897\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.8925 - accuracy: 0.7069\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.8741 - accuracy: 0.7414\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.7795 - accuracy: 0.7241\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.7852 - accuracy: 0.7931\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7392 - accuracy: 0.7414\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.7499 - accuracy: 0.7241\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.6647 - accuracy: 0.7586\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.7467 - accuracy: 0.7069\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.6434 - accuracy: 0.8621\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.5468 - accuracy: 0.8621\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.5777 - accuracy: 0.8103\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.5530 - accuracy: 0.7931\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.5767 - accuracy: 0.7759\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.4211 - accuracy: 0.8448\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5080 - accuracy: 0.8621\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4140 - accuracy: 0.8276\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3710 - accuracy: 0.8793\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.5358 - accuracy: 0.8103\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.3518 - accuracy: 0.8966\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2802 - accuracy: 0.9310\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.3487 - accuracy: 0.8793\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5483 - accuracy: 0.7931\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.3256 - accuracy: 0.8793\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.3076 - accuracy: 0.8793\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.4719 - accuracy: 0.8276\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.2222 - accuracy: 0.9310\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.2375 - accuracy: 0.9310\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1672 - accuracy: 0.9828\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1894 - accuracy: 0.9483\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3087 - accuracy: 0.8966\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.3445 - accuracy: 0.9138\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2657 - accuracy: 0.9138\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.4785 - accuracy: 0.8276\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.2525 - accuracy: 0.9310\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1791 - accuracy: 0.8966\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2914 - accuracy: 0.8966\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.2745 - accuracy: 0.9138\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.2120 - accuracy: 0.9138\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1556 - accuracy: 0.9483\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1324 - accuracy: 0.9655\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2125 - accuracy: 0.9483\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.0684 - accuracy: 0.9828\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1531 - accuracy: 0.9483\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1355 - accuracy: 0.9483\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1059 - accuracy: 0.9310\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1187 - accuracy: 0.9483\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1845 - accuracy: 0.9483\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2036 - accuracy: 0.9655\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0825 - accuracy: 0.9828\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1618 - accuracy: 0.9655\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0689 - accuracy: 0.9655\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2032 - accuracy: 0.9655\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0972 - accuracy: 0.9655\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1036 - accuracy: 0.9655\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0826 - accuracy: 0.9828\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0512 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0874 - accuracy: 0.9655\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0672 - accuracy: 0.9655\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0477 - accuracy: 0.9828\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0802 - accuracy: 0.9655\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0527 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1073 - accuracy: 0.9655\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1090 - accuracy: 0.9483\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0682 - accuracy: 0.9828\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0364 - accuracy: 0.9828\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0841 - accuracy: 0.9655\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0360 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0359 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0449 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0348 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0807 - accuracy: 0.9655\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0956 - accuracy: 0.9828\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0386 - accuracy: 0.9828\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0309 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0955 - accuracy: 0.9655\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0768 - accuracy: 0.9655\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1293 - accuracy: 0.9655\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0178 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0295 - accuracy: 0.9828\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0415 - accuracy: 0.9828\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0194 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0341 - accuracy: 0.9828\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1054 - accuracy: 0.9483\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0931 - accuracy: 0.9828\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0876 - accuracy: 0.9655\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0244 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0402 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0232 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0418 - accuracy: 0.9828\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1150 - accuracy: 0.9655\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0592 - accuracy: 0.9828\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0744 - accuracy: 0.9828\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0250 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1106 - accuracy: 0.9828\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0546 - accuracy: 0.9828\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0373 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0363 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0287 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0632 - accuracy: 0.9655\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0355 - accuracy: 0.9828\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0252 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0168 - accuracy: 0.9828\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0295 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0491 - accuracy: 0.9828\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0474 - accuracy: 0.9828\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0317 - accuracy: 0.9828\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0354 - accuracy: 0.9828\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0744 - accuracy: 0.9828\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0314 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0494 - accuracy: 0.9655\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0794 - accuracy: 0.9483\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0214 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0185 - accuracy: 0.9828\n",
      "Epoch 172/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0273 - accuracy: 0.9828\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0347 - accuracy: 0.9828\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0227 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0403 - accuracy: 0.9828\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0304 - accuracy: 0.9828\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0895 - accuracy: 0.9655\n",
      "Epoch 179/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0144 - accuracy: 0.9828\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0330 - accuracy: 0.9828\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1036 - accuracy: 0.9828\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0268 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0150 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0590 - accuracy: 0.9828\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0340 - accuracy: 0.9828\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0444 - accuracy: 0.9828\n",
      "Epoch 192/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1374 - accuracy: 0.9828\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0183 - accuracy: 0.9828\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0324 - accuracy: 0.9828\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0799 - accuracy: 0.9483\n"
     ]
    }
   ],
   "source": [
    "model_0.compile(loss = \"categorical_crossentropy\",\n",
    "               optimizer = \"adam\",\n",
    "               metrics = [\"accuracy\"])\n",
    "\n",
    "model_0_history = model_0.fit(x = np.array(train_x), y = np.array(train_y), \n",
    "           epochs = 200 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = \"where do you study\"\n",
    "\n",
    "val_in_num = [0] * len(words)\n",
    "for i ,word in enumerate(words):\n",
    "        if word in val:\n",
    "            val_in_num[i] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 161ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.0868992e-08, 3.7168408e-07, 1.0682983e-07, 5.8637460e-07,\n",
       "        3.3047391e-09, 9.9997330e-01, 1.8833718e-05, 3.1905163e-08,\n",
       "        6.7309493e-06]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model_0.predict(np.array([val_in_num]))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int64, numpy=array([5], dtype=int64)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = tf.argmax(pred ,1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['greeting'],\n",
       " ['name'],\n",
       " ['health'],\n",
       " ['situation'],\n",
       " ['address'],\n",
       " ['college'],\n",
       " ['education'],\n",
       " ['goodbye'],\n",
       " ['location']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
